import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout

# è¯»å–æ•°æ®
data = pd.read_csv('D:/cursor/Project/SNS_Project/SNS_Chatbot/Data/dxy_index.csv')
dxy_values = data['Close'].values.reshape(-1, 1)  # å˜æˆåˆ—å‘é‡

# å½’ä¸€åŒ–æ•°æ®
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(dxy_values)

# ä¿®æ­£ `create_dataset()`
def create_dataset(data, time_step=1):
    X, y = [], []
    for i in range(len(data) - time_step):
        X.append(data[i:(i + time_step), 0])  # 30 å¤©å†å²æ•°æ®
        y.append(data[i + time_step, 0])  # é¢„æµ‹å€¼
    return np.array(X), np.array(y).reshape(-1, 1)  # ä¿®æ­£ y å½¢çŠ¶

# è®¾ç½®æ—¶é—´æ­¥é•¿
time_step = 30
X, y = create_dataset(scaled_data, time_step)

# ç¡®ä¿ `X` å½¢çŠ¶ä¸º `(samples, time_steps, features)`
X = X.reshape(X.shape[0], X.shape[1], 1)  # ä¿®æ­£ reshape

# ğŸ”¹ æ‰“å°å½¢çŠ¶æ£€æŸ¥
print("X shape:", X.shape)  # æœŸæœ›: (samples, 30, 1)
print("y shape:", y.shape)  # æœŸæœ›: (samples, 1)

# æ„å»ºæ¨¡å‹
model = Sequential()
model.add(GRU(50, return_sequences=True, input_shape=(time_step, 1)))  # ä¿®æ­£ input_shape
model.add(Dropout(0.2))
model.add(GRU(50, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(1))

# ç¼–è¯‘å’Œè®­ç»ƒæ¨¡å‹
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X, y, epochs=100, batch_size=32)

# ä¿å­˜æ¨¡å‹
model.save('Models/gru_dxy_model.h5')
